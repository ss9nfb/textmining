---
title: "Tidytext"
author: "Nathan Patton"
date: "10/18/2001"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, include=FALSE}
library(tidyverse)
#install.packages("tidytext")
library(tidytext)
#install.packages("ggwordcloud")
library(ggwordcloud)
#install.packages("gutenbergr") 
library(gutenbergr)
#install.packages('textdata')
library(textdata)
#install.packages("textreadr")
library(textreadr)
```

## Loading in the Data

```{r}
LA_times_1 <- read_rtf("C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/CALIFORNIA DROUGHT_ The driest year in a century_ Excep.RTF")
LA_times_1 = tibble(LA_times_1)

LA_times_2 <- read_rtf("C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/PERSPECTIVES_ LETTER FROM WASHINGTON_ There_s still hop.RTF")
LA_times_2 = tibble(LA_times_2)

LA_times_3 <- read_rtf("C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/Hope isn_t a climate-change option -- but activism is (1).RTF")
LA_times_3 = tibble(LA_times_3)

LA_times_4 <- read_rtf("C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/Fan of national parks_ Get ready for them to heat up_ C.RTF")
LA_times_4 = tibble(LA_times_4)

LA_times_5 <- read_rtf("C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/Second Opinion __ BIG IDEAS ON GLOBAL CHALLENGES_ The m.RTF")
LA_times_5 = tibble(LA_times_5)

LA_times_6 <- read_rtf("C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/PERSPECTIVES_ Firms told to come clean on climate change.RTF")
LA_times_6 = tibble(LA_times_6)

LA_times_7 <- read_rtf("C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/Biden pushes climate legislation_ Wrapping up his trip.RTF")
LA_times_7 = tibble(LA_times_7)

LA_times_8 <- read_rtf("C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/PERSPECTIVES_ The dry facts on the drought in the Weste.RTF")
LA_times_8 = tibble(LA_times_8)

LA_times_9 <- read_rtf("C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/Brutal fire season is still _far from over__ Dryness_ h.RTF")
LA_times_9 = tibble(LA_times_9)

LA_times_10 <- read_rtf("C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/The Pacific Northwest melts.RTF")
LA_times_10 = tibble(LA_times_10)
```

## Cleaning Up the Data

```{r, echo=TRUE, eval=FALSE}
#Looks like we have some extra info there at the top and bottom so let's remove those rows.

#1st article 

LA_times_1 <- LA_times_1[19:43, ]

LA_times_1$LA_times_1 <- as.character(LA_times_1$LA_times_1)

#2nd article

LA_times_2 <- LA_times_2[19:42, ]

LA_times_2$LA_times_2 <- as.character(LA_times_2$LA_times_2)

#3rd article

LA_times_3 <- LA_times_3[19:31, ]

LA_times_3$LA_times_3 <- as.character(LA_times_3$LA_times_3)

#4th article

LA_times_4 <- LA_times_4[19:44, ]

LA_times_4$LA_times_4 <- as.character(LA_times_4$LA_times_4)

#5th article

LA_times_5 <- LA_times_5[19:35, ]

LA_times_5$LA_times_5 <- as.character(LA_times_5$LA_times_5)

#6th article 

LA_times_6 <- LA_times_6[19:44, ]

LA_times_6$LA_times_6 <- as.character(LA_times_6$LA_times_6)

#7th article

LA_times_7 <- LA_times_7[20:35, ]

LA_times_7$LA_times_7 <- as.character(LA_times_7$LA_times_7)

#8th article

LA_times_8 <- LA_times_8[19:66, ]

LA_times_8$LA_times_8 <- as.character(LA_times_8$LA_times_8)

#9th article

LA_times_9 <- LA_times_9[19:57, ]

LA_times_9$LA_times_9 <- as.character(LA_times_9$LA_times_9)

#10th article

LA_times_10 <- LA_times_10[18:29, ]

LA_times_10$LA_times_10 <- as.character(LA_times_10$LA_times_10)
```

## Determing word frequencies in each article

```{r, echo=TRUE, eval=FALSE}
#1st article 

LA_times_1 <- LA_times_1 %>%
  unnest_tokens(word, LA_times_1)%>%
  anti_join(stop_words)%>%
  count(word, sort=TRUE)

#2nd article

LA_times_2 <- LA_times_2 %>%
  unnest_tokens(word, LA_times_2)%>%
  anti_join(stop_words)%>%
  count(word, sort=TRUE)

#3rd article

LA_times_3 <- LA_times_3 %>%
  unnest_tokens(word, LA_times_3)%>%
  anti_join(stop_words)%>%
  count(word, sort=TRUE)

#4th article

LA_times_4 <- LA_times_4 %>%
  unnest_tokens(word, LA_times_4)%>%
  anti_join(stop_words)%>%
  count(word, sort=TRUE)

#5th article

LA_times_5 <- LA_times_5 %>%
  unnest_tokens(word, LA_times_5)%>%
  anti_join(stop_words)%>%
  count(word, sort=TRUE)

#6th article 

LA_times_6 <- LA_times_6 %>%
  unnest_tokens(word, LA_times_6)%>%
  anti_join(stop_words)%>%
  count(word, sort=TRUE)

#7th article

LA_times_7 <- LA_times_7 %>%
  unnest_tokens(word, LA_times_7)%>%
  anti_join(stop_words)%>%
  count(word, sort=TRUE)

#8th article

LA_times_8 <- LA_times_8 %>%
  unnest_tokens(word, LA_times_8)%>%
  anti_join(stop_words)%>%
  count(word, sort=TRUE)

#9th article

LA_times_9 <- LA_times_9 %>%
  unnest_tokens(word, LA_times_9)%>%
  anti_join(stop_words)%>%
  count(word, sort=TRUE)

#10th article

LA_times_10 <- LA_times_10 %>%
  unnest_tokens(word, LA_times_10)%>%
  anti_join(stop_words)%>%
  count(word, sort=TRUE)

```

These term frequencies are important, because they needed to be determined in order to help analyze the data and accomplish the task of figuring out patterns in words that occur in the west region of the United States. Refer to the next section to see how the term frequencies help perform sentiment analysis on each of the Los Angeles Times newspaper articles. 

```{r,include=FALSE}
#helps with the sentiment analysis, using package "textdata"
  
get_sentiments('afinn') # we see a list of words and there classification, 2,467 - not really that many overall. 

get_sentiments('nrc') # looks like a good amount more 13,891, but as we can see words are classified in several different categories. 

get_sentiments('bing') # looks like a good amount more 6,776, but as we can see just negative and positive. 

LA_Times_1_sentiment_affin <- LA_times_1 %>%
  inner_join(get_sentiments("afinn"))#using a inner join to match words and add the sentiment variable

LA_Times_1_sentiment_nrc <- LA_times_1 %>%
  inner_join(get_sentiments("nrc"))

LA_Times_1_sentiment_bing <- LA_times_1 %>%
  inner_join(get_sentiments("bing"))

#Walk through the same process with Article 2

LA_Times_2_sentiment_affin <- LA_times_2 %>%
  inner_join(get_sentiments("afinn"))#using a inner join to match words and add the sentiment variable

LA_Times_2_sentiment_nrc <- LA_times_2 %>%
  inner_join(get_sentiments("nrc"))

LA_Times_2_sentiment_bing <- LA_times_2 %>%
  inner_join(get_sentiments("bing"))

#Walk through the same process with Article 3

LA_Times_3_sentiment_affin <- LA_times_3 %>%
  inner_join(get_sentiments("afinn"))#using a inner join to match words and add the sentiment variable

LA_Times_3_sentiment_nrc <- LA_times_3 %>%
  inner_join(get_sentiments("nrc"))

LA_Times_3_sentiment_bing <- LA_times_3 %>%
  inner_join(get_sentiments("bing"))

#Walk through the same process with Article 4

LA_Times_4_sentiment_affin <- LA_times_4 %>%
  inner_join(get_sentiments("afinn"))#using a inner join to match words and add the sentiment variable

LA_Times_4_sentiment_nrc <- LA_times_4 %>%
  inner_join(get_sentiments("nrc"))

LA_Times_4_sentiment_bing <- LA_times_4 %>%
  inner_join(get_sentiments("bing"))

#Walk through the same process with Article 5

LA_Times_5_sentiment_affin <- LA_times_5 %>%
  inner_join(get_sentiments("afinn"))#using a inner join to match words and add the sentiment variable

LA_Times_5_sentiment_nrc <- LA_times_5 %>%
  inner_join(get_sentiments("nrc"))

LA_Times_5_sentiment_bing <- LA_times_5 %>%
  inner_join(get_sentiments("bing"))

#Walk through the same process with Article 6

LA_Times_6_sentiment_affin <- LA_times_6 %>%
  inner_join(get_sentiments("afinn"))#using a inner join to match words and add the sentiment variable

LA_Times_6_sentiment_nrc <- LA_times_6 %>%
  inner_join(get_sentiments("nrc"))

LA_Times_6_sentiment_bing <- LA_times_6 %>%
  inner_join(get_sentiments("bing"))

#Walk through the same process with Article 7

LA_Times_7_sentiment_affin <- LA_times_7 %>%
  inner_join(get_sentiments("afinn"))#using a inner join to match words and add the sentiment variable

LA_Times_7_sentiment_nrc <- LA_times_7 %>%
  inner_join(get_sentiments("nrc"))

LA_Times_7_sentiment_bing <- LA_times_7 %>%
  inner_join(get_sentiments("bing"))

#Walk through the same process with Article 8

LA_Times_8_sentiment_affin <- LA_times_8 %>%
  inner_join(get_sentiments("afinn"))#using a inner join to match words and add the sentiment variable

LA_Times_8_sentiment_nrc <- LA_times_8 %>%
  inner_join(get_sentiments("nrc"))

LA_Times_8_sentiment_bing <- LA_times_8 %>%
  inner_join(get_sentiments("bing"))

#Walk through the same process with Article 9
LA_Times_9_sentiment_affin <- LA_times_9 %>%
  inner_join(get_sentiments("afinn"))#using a inner join to match words and add the sentiment variable

LA_Times_9_sentiment_nrc <- LA_times_9 %>%
  inner_join(get_sentiments("nrc"))

LA_Times_9_sentiment_bing <- LA_times_9 %>%
  inner_join(get_sentiments("bing"))

#Walk through the same process with Article 10

LA_Times_10_sentiment_affin <- LA_times_10 %>%
  inner_join(get_sentiments("afinn"))#using a inner join to match words and add the sentiment variable

LA_Times_10_sentiment_nrc <- LA_times_10 %>%
  inner_join(get_sentiments("nrc"))

LA_Times_10_sentiment_bing <- LA_times_10 %>%
  inner_join(get_sentiments("bing"))

```

Now that we have our sentiment let's do some quick comparisons

## Comparing the Sentiment Analysis Between Articles 

### Using Tabling on Each Article

```{r, include=FALSE}

#We can just do some tabling to see the differences in bing and nrc

table(LA_Times_1_sentiment_bing$sentiment)
table(LA_Times_2_sentiment_bing$sentiment)
table(LA_Times_3_sentiment_bing$sentiment)
table(LA_Times_4_sentiment_bing$sentiment)
table(LA_Times_5_sentiment_bing$sentiment)
table(LA_Times_6_sentiment_bing$sentiment)
table(LA_Times_7_sentiment_bing$sentiment)
table(LA_Times_8_sentiment_bing$sentiment)
table(LA_Times_9_sentiment_bing$sentiment)
table(LA_Times_10_sentiment_bing$sentiment)

table(LA_Times_1_sentiment_nrc$sentiment)
table(LA_Times_2_sentiment_nrc$sentiment)
table(LA_Times_3_sentiment_nrc$sentiment)
table(LA_Times_4_sentiment_nrc$sentiment)
table(LA_Times_5_sentiment_nrc$sentiment)
table(LA_Times_6_sentiment_nrc$sentiment)
table(LA_Times_7_sentiment_nrc$sentiment)
table(LA_Times_8_sentiment_nrc$sentiment)
table(LA_Times_9_sentiment_nrc$sentiment)
table(LA_Times_10_sentiment_nrc$sentiment)
```

### Using Histogram on Each Article

```{r, include=FALSE}

ggplot(data = LA_Times_1_sentiment_affin, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("LA_Times_1 Sentiment Range")+
  theme_minimal()

ggplot(data = LA_Times_2_sentiment_affin, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("LA_Times_2 Sentiment Range")+
  theme_minimal()

ggplot(data = LA_Times_3_sentiment_affin, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("LA_Times_3 Sentiment Range")+
  theme_minimal()

ggplot(data = LA_Times_4_sentiment_affin, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("LA_Times_4 Sentiment Range")+
  theme_minimal()

ggplot(data = LA_Times_5_sentiment_affin, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("LA_Times_5 Sentiment Range")+
  theme_minimal()

ggplot(data = LA_Times_6_sentiment_affin, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("LA_Times_6 Sentiment Range")+
  theme_minimal()

ggplot(data = LA_Times_7_sentiment_affin, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("LA_Times_7 Sentiment Range")+
  theme_minimal()

ggplot(data = LA_Times_8_sentiment_affin, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("LA_Times_8 Sentiment Range")+
  theme_minimal()

ggplot(data = LA_Times_9_sentiment_affin, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("LA_Times_9 Sentiment Range")+
  theme_minimal()

ggplot(data = LA_Times_10_sentiment_affin, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("LA_Times_10 Sentiment Range")+
  theme_minimal()
```

### Using Word Cloud on Each Article 

```{r, echo =TRUE, eval=FALSE}

set.seed(42)
ggplot(LA_times_1[1:50,], aes(label = word, size = n)
       ) +
  geom_text_wordcloud() +
  theme_minimal()

ggplot(LA_times_2[1:50,], aes(label = word, size = n)
       ) +
  geom_text_wordcloud() +
  theme_minimal()

ggplot(LA_times_3[1:50,], aes(label = word, size = n)
       ) +
  geom_text_wordcloud() +
  theme_minimal()

ggplot(LA_times_4[1:50,], aes(label = word, size = n)
       ) +
  geom_text_wordcloud() +
  theme_minimal()

ggplot(LA_times_5[1:50,], aes(label = word, size = n)
       ) +
  geom_text_wordcloud() +
  theme_minimal()

ggplot(LA_times_6[1:50,], aes(label = word, size = n)
       ) +
  geom_text_wordcloud() +
  theme_minimal()

ggplot(LA_times_7[1:50,], aes(label = word, size = n)
       ) +
  geom_text_wordcloud() +
  theme_minimal()

ggplot(LA_times_8[1:50,], aes(label = word, size = n)
       ) +
  geom_text_wordcloud() +
  theme_minimal()

ggplot(LA_times_9[1:50,], aes(label = word, size = n)
       ) +
  geom_text_wordcloud() +
  theme_minimal()

ggplot(LA_times_10[1:50,], aes(label = word, size = n)
       ) +
  geom_text_wordcloud() +
  theme_minimal()
```

## Determining the overall tf-idf for LA Times Newspaper 

```{r, warning=FALSE, message=FALSE}

#need to the raw data again

LA_times_1 <- read_rtf("C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/CALIFORNIA DROUGHT_ The driest year in a century_ Excep.RTF")
LA_times_1 = tibble(LA_times_1)
LA_times_1 <- LA_times_1[19:43, ]
LA_times_1$LA_times_1 <- as.character(LA_times_1$LA_times_1)

LA_times_2 <- read_rtf("C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/PERSPECTIVES_ LETTER FROM WASHINGTON_ There_s still hop.RTF")
LA_times_2 = tibble(LA_times_2)
LA_times_2 <- LA_times_2[19:42, ]
LA_times_2$LA_times_2 <- as.character(LA_times_2$LA_times_2)

LA_times_3 <- read_rtf("C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/Hope isn_t a climate-change option -- but activism is (1).RTF")
LA_times_3 = tibble(LA_times_3)
LA_times_3 <- LA_times_3[19:31, ]
LA_times_3$LA_times_3 <- as.character(LA_times_3$LA_times_3)

LA_times_4 <- read_rtf("C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/Fan of national parks_ Get ready for them to heat up_ C.RTF")
LA_times_4 = tibble(LA_times_4)
LA_times_4 <- LA_times_4[19:44, ]
LA_times_4$LA_times_4 <- as.character(LA_times_4$LA_times_4)

LA_times_5 <- read_rtf("C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/Second Opinion __ BIG IDEAS ON GLOBAL CHALLENGES_ The m.RTF")
LA_times_5 = tibble(LA_times_5)
LA_times_5 <- LA_times_5[19:35, ]
LA_times_5$LA_times_5 <- as.character(LA_times_5$LA_times_5)

LA_times_6 <- read_rtf("C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/PERSPECTIVES_ Firms told to come clean on climate change.RTF")
LA_times_6 = tibble(LA_times_6)
LA_times_6 <- LA_times_6[19:44, ]
LA_times_6$LA_times_6 <- as.character(LA_times_6$LA_times_6)

LA_times_7 <- read_rtf("C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/Biden pushes climate legislation_ Wrapping up his trip.RTF")
LA_times_7 = tibble(LA_times_7)
LA_times_7 <- LA_times_7[20:35, ]
LA_times_7$LA_times_7 <- as.character(LA_times_7$LA_times_7)

LA_times_8 <- read_rtf("C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/PERSPECTIVES_ The dry facts on the drought in the Weste.RTF")
LA_times_8 = tibble(LA_times_8)
LA_times_8 <- LA_times_8[19:66, ]
LA_times_8$LA_times_8 <- as.character(LA_times_8$LA_times_8)

LA_times_9 <- read_rtf("C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/Brutal fire season is still _far from over__ Dryness_ h.RTF")
LA_times_9 = tibble(LA_times_9)
LA_times_9 <- LA_times_9[19:57, ]
LA_times_9$LA_times_9 <- as.character(LA_times_9$LA_times_9)

LA_times_10 <- read_rtf("C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/The Pacific Northwest melts.RTF")
LA_times_10 = tibble(LA_times_10)
LA_times_10 <- LA_times_10[18:29, ]
LA_times_10$LA_times_10 <- as.character(LA_times_10$LA_times_10)

#Corpus is all the articles, documents are each of the articles, and token is the words 

data_prep <- function(x,y,z){
  i <- as_tibble(t(x))
  ii <- unite(i,"text",y:z,remove = TRUE,sep = "")
}

LA_times_1_bag <- data_prep(LA_times_1,'V1','V25')

LA_times_2_bag <- data_prep(LA_times_2,'V1','V24')

LA_times_3_bag <- data_prep(LA_times_3,'V1','V13')

LA_times_4_bag <- data_prep(LA_times_4,'V1','V26')

LA_times_5_bag <- data_prep(LA_times_5,'V1','V17')

LA_times_6_bag <- data_prep(LA_times_6,'V1','V26')

LA_times_7_bag <- data_prep(LA_times_7,'V1','V16')

LA_times_8_bag <- data_prep(LA_times_8,'V1','V48')

LA_times_9_bag <- data_prep(LA_times_9,'V1','V39')

LA_times_10_bag <- data_prep(LA_times_10,'V1','V12')

articles <- c("LATimes1","LATimes2","LATimes3","LATimes4","LATimes5","LATimes6","LATimes7","LATimes8","LATimes9","LATimes10")

tf_idf <- tibble(articles,text=t(tibble(LA_times_1_bag,LA_times_2_bag,LA_times_3_bag,LA_times_4_bag,LA_times_5_bag,LA_times_6_bag,LA_times_7_bag,LA_times_8_bag,LA_times_9_bag,LA_times_10_bag,.name_repair = "universal")))

View(tf_idf)

word_count <- tf_idf %>%
  unnest_tokens(word, text) %>%
  count(articles, word, sort = TRUE)

total_words <- word_count %>% 
  group_by(articles) %>% 
  summarize(total = sum(n))

newspaper_words <- left_join(word_count, total_words)

newspaper_words <- newspaper_words %>%
  anti_join(stop_words)%>%
  bind_tf_idf(word, articles, n)

view(newspaper_words)

write.csv(newspaper_words,"C:/Users/natha/OneDrive/Documents/DS-3001/My Projects/Text Mining/west.csv")
  
```

The tf_idf was calculated for the words in these articles, using tf_idf = tf*idf, which shows the general pattern of importance of certain words. To clarify, tf is the term frequency, and idf is the inverse document frequency. The importance and tf_idf are directly related, meaning the higher the value of the tf_idf, the more important the word is in the article. Therefore, the most important words that make sense in this situtaiton in the western region (Los Angeles Times) are words such as lands, investors, water, fire, visitation, defense, military, breaking, park, etc. When the tf_idf value were arranged in descending order, it was found that the highest tf_idf value was 0.05055576 for land, which totally makes sense. Climate change has had a significant effect on our land, so it makes sense that "land" is the most important word. 

## Sentiment Analysis on Newspaper

### Using Tabling on the Newspaper

```{r}
newspaper_words_sentiment_nrc <- newspaper_words %>%
  inner_join(get_sentiments("nrc"))

newspaper_words_sentiment_bing <- newspaper_words %>%
  inner_join(get_sentiments("bing"))

table(newspaper_words_sentiment_bing$sentiment)
table(newspaper_words_sentiment_nrc$sentiment)
```

For most of the articles, there is a significant gap between the number of positive and negative words, which is seen in the output of each of the bing sentiment analysis. Majority of the articles (7 out of the 10) include more negative words than positive words, 213 negative vs 117 positive in total, which is almost a 2:1 ration, and that makes sense based on the topic of the articles. Climate change is a very huge issue that takes place in the world, and the best way to get people to realize this is by raising the concern and making it clear that something must be done. On top of that, by comparing the results of the nrc sentiment analysis, it can be concluded that many of the words represent fear, trust, and anticipation, as they are the top 3 most counted for words. 

### Using Histogram on the Newspaper

```{r}
newspaper_words_sentiment_affin <- newspaper_words %>%
  inner_join(get_sentiments("afinn"))
  
ggplot(data = newspaper_words_sentiment_affin, 
       aes(x=value)
        )+
  geom_histogram()+
  ggtitle("LA Times Sentiment Range")+
  theme_minimal()
```

It can be concluded that the histogram agrees with the nrc and bing sentiment analysis. This can be seen by the high negativity, higher word count below 0, and low positivity, lower word count above 0. Therefore, the words are more skewed to the left, indicating that the western region articles tend to be more negative. 

### Using Word Cloud on the Newspaper

```{r}

ggplot(newspaper_words[1:50,], aes(label = word, size = n)
       ) +
  geom_text_wordcloud() +
  theme_minimal()

```

Based on the output of the word cloud that corresponds to the LA Times newspaper, which in this case is the corpus formed by combining the articles (documents), the most commonly said words are those that appear bolder, larger, an more often. Therefore, it seems that words such as "drought", "water","fire", and "climate" are mentioned quite often in Los Angeles Times Newspaper articles about climate change. This completely makes sense because in the western region, especially California, they often suffer from droughts, and wild fires. One of the most surprising words for me was "change" as I thought this word would possibly appear bolder and larger, because it would make sense that this word would be used often in articles about climate CHANGE. 
